---
title: "PyTorch æŸå¤±å‡½æ•°å…¨è§£ï¼šä»æ•°å­¦åŸç†åˆ°å·¥ç¨‹å®è·µ"
date: "2026-02-25"
category: "Deep Learning"
tags: ["PyTorch", "Loss Function", "Mathematics", "Best Practices"]
---

# PyTorch æŸå¤±å‡½æ•°å…¨è§£ï¼šä»æ•°å­¦åŸç†åˆ°å·¥ç¨‹å®è·µ ğŸš€

åœ¨æ·±åº¦å­¦ä¹ çš„æµ©ç€šå®‡å®™ä¸­ï¼Œ**æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰** æ‰®æ¼”ç€â€œåŒ—ææ˜Ÿâ€çš„è§’è‰² ğŸŒŸã€‚å®ƒé‡åŒ–äº†æ¨¡å‹é¢„æµ‹ä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®è·ï¼ŒæŒ‡å¼•ç€ä¼˜åŒ–å™¨ï¼ˆOptimizerï¼‰åœ¨å‚æ•°ç©ºé—´ä¸­å¯»æ‰¾æœ€ä¼˜è§£ã€‚

> ğŸ’¡ **æ ¸å¿ƒè§‚ç‚¹**ï¼šé€‰æ‹©åˆé€‚çš„æŸå¤±å‡½æ•°ï¼Œå¾€å¾€æ¯”è°ƒæ•´ç½‘ç»œç»“æ„æ›´èƒ½å†³å®šæ¨¡å‹çš„æœ€ç»ˆæ€§èƒ½ã€‚

æœ¬æ–‡å°†æ·±å…¥å‰–æ PyTorch `torch.nn` æ¨¡å—ä¸­å†…ç½®çš„å…¨éƒ¨æŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬å°†è¶…è¶Šç®€å•çš„ API è°ƒç”¨ï¼Œæ·±å…¥å…¶ **æ•°å­¦æœ¬è´¨**ã€**æ¢¯åº¦ç‰¹æ€§**ã€**æ•°å€¼ç¨³å®šæ€§** ä»¥åŠ **å·¥ç¨‹é™·é˜±**ã€‚æ— è®ºä½ æ˜¯å¤„ç†å›å½’ã€åˆ†ç±»ã€æ’åºè¿˜æ˜¯ç”Ÿæˆä»»åŠ¡ï¼Œæœ¬æ–‡éƒ½å°†ä¸ºä½ æä¾›è¯¦å°½çš„æŒ‡å¯¼ã€‚

---

## ğŸ“š ç›®å½•

1.  [**å›å½’æŸå¤± (Regression Losses)**](#1-å›å½’æŸå¤±-regression-losses)
    *   [L1Loss (MAE)](#11-nnl1loss)
    *   [MSELoss (L2)](#12-nnmseloss)
    *   [HuberLoss](#13-nnhuberloss)
    *   [SmoothL1Loss](#14-nnsmoothl1loss)
2.  [**åˆ†ç±»æŸå¤± (Classification Losses)**](#2-åˆ†ç±»æŸå¤±-classification-losses)
    *   [NLLLoss](#21-nnnllloss)
    *   [CrossEntropyLoss](#22-nncrossentropyloss)
    *   [BCELoss](#23-nnbceloss)
    *   [BCEWithLogitsLoss](#24-nnbcewithlogitsloss)
    *   [å¤šæ ‡ç­¾ä¸ Margin æŸå¤±](#24-å¤šæ ‡ç­¾ä¸-margin-æŸå¤±)
3.  [**åµŒå…¥ä¸æ’åºæŸå¤± (Embedding & Ranking Losses)**](#3-åµŒå…¥ä¸æ’åºæŸå¤±-embedding--ranking-losses)
    *   [MarginRankingLoss](#31-nnmarginrankingloss)
    *   [CosineEmbeddingLoss](#32-nncosineembeddingloss)
    *   [TripletMarginLoss](#33-nntripletmarginloss)
4.  [**åˆ†å¸ƒæ•£åº¦ä¸åºåˆ—æŸå¤± (Distribution & Sequence Losses)**](#4-åˆ†å¸ƒæ•£åº¦ä¸åºåˆ—æŸå¤±-distribution--sequence-losses)
    *   [KLDivLoss](#41-nnkldivloss)
    *   [CTCLoss](#42-nnctcloss)
5.  [**æ€»ç»“ä¸é€ŸæŸ¥è¡¨**](#5-æ€»ç»“ä¸é€ŸæŸ¥è¡¨)

---

## 1. å›å½’æŸå¤± (Regression Losses) ğŸ“ˆ

å›å½’ä»»åŠ¡çš„ç›®æ ‡æ˜¯é¢„æµ‹è¿ç»­æ•°å€¼ï¼Œå¦‚æˆ¿ä»·é¢„æµ‹ã€åæ ‡å›å½’ç­‰ã€‚

### 1.1 `nn.L1Loss`

**å…¨ç§°**ï¼šMean Absolute Error (MAE)

#### ğŸ“ æ•°å­¦å®šä¹‰
å¯¹äºè¾“å…¥ $x$ å’Œç›®æ ‡ $y$ï¼ŒåŒ…å« $N$ ä¸ªæ ·æœ¬ï¼š

$$
\ell(x, y) = L = \{l_1, \dots, l_N\}^\top, \quad l_n = |x_n - y_n|
$$

è‹¥ `reduction='mean'`ï¼ˆé»˜è®¤ï¼‰ï¼š
$$
\ell(x, y) = \text{mean}(L)
$$

#### ğŸ” å‡½æ•°ç‰¹å¾
*   **æ¢¯åº¦ç‰¹æ€§**ï¼šæ¢¯åº¦åœ¨éé›¶å¤„æ’ä¸º $\pm 1$ï¼ˆæˆ–ç¼©æ”¾å› å­ï¼‰ï¼Œåœ¨ $0$ å¤„ä¸å¯å¯¼ï¼ˆé€šå¸¸æ¬¡æ¢¯åº¦è®¾ä¸º 0ï¼‰ã€‚
*   **é²æ£’æ€§**ï¼šå¯¹å¼‚å¸¸å€¼ï¼ˆOutliersï¼‰**éå¸¸é²æ£’**ã€‚å› ä¸ºè¯¯å·®å¢å¤§æ—¶ï¼Œæ¢¯åº¦ä¸ä¼šåƒ MSE é‚£æ ·å‘ˆäºŒæ¬¡æ–¹å¢é•¿ã€‚
*   **ç¨€ç–æ€§**ï¼šå€¾å‘äºäº§ç”Ÿç¨€ç–è§£ï¼ˆæŸäº›ç»´åº¦è¯¯å·®å®Œå…¨ä¸º 0ï¼‰ã€‚

#### ğŸ› ï¸ é€‚ç”¨åœºæ™¯
*   å›å½’ä»»åŠ¡ä¸­å­˜åœ¨è¾ƒå¤šå¼‚å¸¸å€¼ã€‚
*   å›¾åƒç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚ GAN ä¸­çš„é‡æ„æŸå¤±ï¼‰ï¼ŒL1 æ¯” L2 äº§ç”Ÿçš„å›¾åƒæ›´æ¸…æ™°ï¼ˆL2 å€¾å‘äºæ¨¡ç³Šå¹³å‡ï¼‰ã€‚

> âš ï¸ **æ³¨æ„äº‹é¡¹**ï¼šç”±äºæ¢¯åº¦æ’å®šï¼Œåœ¨æ¥è¿‘æœ€ä¼˜è§£æ—¶ï¼ˆè¯¯å·®å¾ˆå°ï¼‰ï¼Œæ¢¯åº¦ä¾ç„¶å¾ˆå¤§ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹åœ¨æœ€ä¼˜è§£é™„è¿‘éœ‡è¡ã€‚é€šå¸¸éœ€è¦é…åˆ **å­¦ä¹ ç‡è¡°å‡ï¼ˆLearning Rate Decayï¼‰** ä½¿ç”¨ã€‚

#### ğŸ’» å®Œæ•´ç¤ºä¾‹
```python
import torch
import torch.nn as nn

# 1. æ•°æ®æ„é€ 
# é¢„æµ‹å€¼ (Batch=2, Dim=3)
input_tensor = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], requires_grad=True)
# çœŸå®å€¼
target_tensor = torch.tensor([[1.5, 2.0, 4.5], [3.0, 5.0, 8.0]])

# 2. å®šä¹‰æŸå¤±å‡½æ•°
# reduction='mean' (é»˜è®¤): è¿”å›æ ‡é‡ï¼Œæ‰€æœ‰å…ƒç´ è¯¯å·®çš„å¹³å‡å€¼
criterion = nn.L1Loss(reduction='mean')

# 3. å‰å‘è®¡ç®—
loss = criterion(input_tensor, target_tensor)
print(f"L1 Loss Value: {loss.item()}") 
# æ‰‹åŠ¨è®¡ç®—éªŒè¯: (|1-1.5| + |2-2| + |3-4.5| + |4-3| + |5-5| + |6-8|) / 6 
# = (0.5 + 0 + 1.5 + 1.0 + 0 + 2.0) / 6 = 5.0 / 6 â‰ˆ 0.8333

# 4. åå‘ä¼ æ’­
loss.backward()
print(f"Input Gradients:\n{input_tensor.grad}")
# æ¢¯åº¦åº”ä¸º +1/6, -1/6 æˆ– 0 (å–å†³äº x>y è¿˜æ˜¯ x<y)
```

---

### 1.2 `nn.MSELoss`

**å…¨ç§°**ï¼šMean Squared Error (L2 Loss)

#### ğŸ“ æ•°å­¦å®šä¹‰
$$
l_n = (x_n - y_n)^2
$$

#### ğŸ” å‡½æ•°ç‰¹å¾
*   **æ¢¯åº¦ç‰¹æ€§**ï¼šæ¢¯åº¦éšè¯¯å·®çº¿æ€§å˜åŒ– $\frac{\partial l}{\partial x} \propto (x-y)$ã€‚è¯¯å·®è¶Šå¤§ï¼Œæ¢¯åº¦è¶Šå¤§ï¼›è¯¯å·®è¶Šå°ï¼Œæ¢¯åº¦è¶Šå°ã€‚
*   **å‡¸æ€§**ï¼šä¸¥æ ¼å‡¸å‡½æ•°ï¼Œæœ‰å”¯ä¸€çš„å…¨å±€æœ€ä¼˜è§£ã€‚
*   **æ•æ„Ÿåº¦**ï¼šå¯¹å¼‚å¸¸å€¼**æåº¦æ•æ„Ÿ**ã€‚ä¸€ä¸ªå·¨å¤§çš„è¯¯å·®å¹³æ–¹åä¼šä¸»å¯¼æ•´ä¸ª Lossï¼Œå¯¼è‡´æ¨¡å‹è¿‡åº¦è¿å°±å¼‚å¸¸ç‚¹ã€‚

#### ğŸ› ï¸ é€‚ç”¨åœºæ™¯
*   é€šç”¨çš„å›å½’ä»»åŠ¡ã€‚
*   å‡è®¾å™ªå£°æœä»é«˜æ–¯åˆ†å¸ƒçš„æ•°æ®ã€‚

> âš ï¸ **æ³¨æ„äº‹é¡¹**ï¼šå¦‚æœé¢„æµ‹å€¼å’Œç›®æ ‡å€¼éå¸¸å¤§ï¼Œå¹³æ–¹åå¯èƒ½å¯¼è‡´ float32 æº¢å‡ºï¼ˆNaNï¼‰ã€‚å»ºè®®å¯¹æ•°æ®è¿›è¡Œ **æ ‡å‡†åŒ–ï¼ˆNormalizationï¼‰**ã€‚

#### ğŸ’» å®Œæ•´ç¤ºä¾‹
```python
import torch
import torch.nn as nn

input_tensor = torch.randn(2, 3, requires_grad=True)
target_tensor = torch.randn(2, 3)

criterion = nn.MSELoss()
loss = criterion(input_tensor, target_tensor)
loss.backward()

print(f"MSE Loss: {loss.item()}")
# æ£€æŸ¥æ¢¯åº¦èŒƒæ•°
print(f"Gradient Norm: {input_tensor.grad.norm().item()}")
```

---

### 1.3 `nn.HuberLoss`

#### ğŸ“ æ•°å­¦å®šä¹‰
ç»“åˆäº† L1 å’Œ L2 çš„ä¼˜ç‚¹ã€‚åœ¨è¯¯å·®è¾ƒå°æ—¶è¡¨ç°ä¸º MSEï¼ˆå¹³æ»‘å¯å¯¼ï¼‰ï¼Œåœ¨è¯¯å·®è¾ƒå¤§æ—¶è¡¨ç°ä¸º L1ï¼ˆé²æ£’ï¼‰ã€‚

$$
l_n = \begin{cases}
0.5 (x_n - y_n)^2 & \text{if } |x_n - y_n| < \delta \\
\delta (|x_n - y_n| - 0.5 \delta) & \text{otherwise}
\end{cases}
$$

å…¶ä¸­ $\delta$ (`delta`) æ˜¯é˜ˆå€¼å‚æ•°ã€‚

#### ğŸ” å‡½æ•°ç‰¹å¾
*   **å¹³æ»‘æ€§**ï¼šåœ¨ 0 å¤„ä¸€é˜¶å¯å¯¼ï¼ˆä¸åƒ L1ï¼‰ã€‚
*   **é²æ£’æ€§**ï¼šå¤§è¯¯å·®åŒºåŸŸæ¢¯åº¦å—é™ï¼ˆä¸åƒ MSEï¼‰ã€‚

#### ğŸ› ï¸ é€‚ç”¨åœºæ™¯
*   å›å½’ä»»åŠ¡ï¼Œä¸”æ•°æ®ä¸­å¯èƒ½åŒ…å«ç¦»ç¾¤ç‚¹ï¼ˆOutliersï¼‰ã€‚
*   ç›®æ ‡æ£€æµ‹ä¸­çš„ Bounding Box å›å½’ï¼ˆå¦‚ Fast R-CNN æ—©æœŸç‰ˆæœ¬ï¼‰ã€‚

> ğŸ’¡ **Pro Tip**ï¼š`delta` çš„é€‰æ‹©è‡³å…³é‡è¦ã€‚é€šå¸¸éœ€è¦æ ¹æ®æ•°æ®åˆ†å¸ƒè°ƒæ•´ã€‚PyTorch é»˜è®¤ `delta=1.0`ã€‚

#### ğŸ’» å®Œæ•´ç¤ºä¾‹
```python
import torch
import torch.nn as nn

x = torch.tensor([0.1, 5.0], requires_grad=True) # 0.1 æ˜¯å°è¯¯å·®, 5.0 æ˜¯å¤§è¯¯å·®
y = torch.tensor([0.0, 0.0])

criterion = nn.HuberLoss(delta=1.0)
loss = criterion(x, y)

# éªŒè¯:
# ç¬¬ä¸€ä¸ªå…ƒç´  |0.1| < 1.0 -> 0.5 * 0.1^2 = 0.005
# ç¬¬äºŒä¸ªå…ƒç´  |5.0| >= 1.0 -> 1.0 * (5.0 - 0.5*1.0) = 4.5
# Mean = (0.005 + 4.5) / 2 = 2.2525
print(f"Huber Loss: {loss.item()}")
```

---

### 1.4 `nn.SmoothL1Loss`

**å¤‡æ³¨**ï¼šåœ¨ PyTorch æ—©æœŸç‰ˆæœ¬ä¸­å¹¿æ³›ä½¿ç”¨ï¼Œæ•°å­¦å½¢å¼ä¸Šä¸ `HuberLoss` éå¸¸ç›¸ä¼¼ï¼Œä½†å‚æ•°åŒ–æ–¹å¼ç•¥æœ‰ä¸åŒï¼ˆ`beta` å‚æ•°ï¼‰ã€‚

#### ğŸ“ æ•°å­¦å®šä¹‰
$$
l_n = \begin{cases}
0.5 (x_n - y_n)^2 / \beta & \text{if } |x_n - y_n| < \beta \\
|x_n - y_n| - 0.5 \beta & \text{otherwise}
\end{cases}
$$

> ğŸ”„ **å¯¹æ¯”**ï¼š`SmoothL1Loss` é€šå¸¸é»˜è®¤ `beta=1.0`ã€‚å½“ `beta=1.0` ä¸” `delta=1.0` æ—¶ï¼Œä¸¤è€…ç­‰ä»·ã€‚æ¨èåœ¨æ–°ä»£ç ä¸­ä¼˜å…ˆä½¿ç”¨ `HuberLoss`ï¼Œè¯­ä¹‰æ›´æ˜ç¡®ã€‚

---

## 2. åˆ†ç±»æŸå¤± (Classification Losses) ğŸ·ï¸

åˆ†ç±»ä»»åŠ¡æ¶‰åŠç¦»æ•£æ ‡ç­¾çš„é¢„æµ‹ã€‚

### 2.1 `nn.NLLLoss`

**å…¨ç§°**ï¼šNegative Log Likelihood Loss

#### ğŸ“ æ•°å­¦å®šä¹‰
$$
\ell(x, y) = L = \{l_1, \dots, l_N\}^\top, \quad l_n = -w_{y_n} x_{n, y_n}
$$
å…¶ä¸­ $x$ æ˜¯è¾“å…¥ï¼Œ$y$ æ˜¯ç±»åˆ«ç´¢å¼•ã€‚æ³¨æ„ï¼Œ$x$ **å¿…é¡»æ˜¯**å¯¹æ•°æ¦‚ç‡ï¼ˆLog-Probabilitiesï¼‰ã€‚

#### ğŸ› ï¸ é€‚ç”¨åœºæ™¯
*   å¤šåˆ†ç±»ä»»åŠ¡ï¼Œä¸”æ¨¡å‹æœ€åä¸€å±‚å·²ç»ä½¿ç”¨äº† `nn.LogSoftmax`ã€‚

> âš ï¸ **å¸¸è§é™·é˜±**ï¼šæ–°æ‰‹æœ€å®¹æ˜“çŠ¯çš„é”™è¯¯æ˜¯ç›´æ¥ä¼ å…¥ Logitsï¼ˆæœªå½’ä¸€åŒ–çš„åˆ†æ•°ï¼‰æˆ– Softmax åçš„æ¦‚ç‡ã€‚**å¿…é¡»ä¼ å…¥ LogSoftmax çš„ç»“æœ**ã€‚è¾“å…¥åº”è¯¥æ˜¯è´Ÿæ•°ï¼ˆå› ä¸ºæ˜¯ log æ¦‚ç‡ï¼‰ï¼ŒèŒƒå›´ $(-\infty, 0]$ã€‚

---

### 2.2 `nn.CrossEntropyLoss`

**åœ°ä½**ï¼šåˆ†ç±»ä»»åŠ¡çš„â€œé»˜è®¤é€‰æ‹©â€ ğŸ‘‘ã€‚

#### ğŸ“ æ•°å­¦å®šä¹‰
PyTorch çš„ `CrossEntropyLoss` å®é™…ä¸Šæ˜¯ `nn.LogSoftmax` å’Œ `nn.NLLLoss` çš„ç»„åˆã€‚

$$
\text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right) = -x[class] + \log\left(\sum_j \exp(x[j])\right)
$$

#### ğŸ” å‡½æ•°ç‰¹å¾
*   **æ•°å€¼ç¨³å®šæ€§**ï¼šå†…éƒ¨ä½¿ç”¨ Log-Sum-Exp æŠ€å·§ï¼ˆ`log_softmax`ï¼‰ï¼Œé¿å…äº†ç›´æ¥è®¡ç®— $\exp(x)$ å¯èƒ½å¯¼è‡´çš„æº¢å‡ºé—®é¢˜ã€‚
*   **æ¢¯åº¦ç‰¹æ€§**ï¼š$\frac{\partial L}{\partial x_i} = p_i - y_i$ï¼ˆå…¶ä¸­ $p$ æ˜¯ softmax æ¦‚ç‡ï¼‰ã€‚è¿™æ„å‘³ç€æ¢¯åº¦ç›´æ¥å–å†³äºé¢„æµ‹æ¦‚ç‡ä¸çœŸå®æ ‡ç­¾çš„è¯¯å·®ï¼Œéå¸¸ç›´è§‚ä¸”æ¢¯åº¦è¡¨ç°è‰¯å¥½ã€‚

#### ğŸ› ï¸ é€‚ç”¨åœºæ™¯
*   å¤šåˆ†ç±»ä»»åŠ¡ï¼ˆImageNet, NLP åˆ†ç±»ï¼‰ã€‚

#### ğŸ’¡ å…³é”®å‚æ•°
*   **`weight` å‚æ•°**ï¼šå¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼ˆClass Imbalanceï¼‰æ—¶çš„ç¥å™¨ã€‚å¯ä»¥ç»™å°‘æ ·æœ¬ç±»åˆ«èµ‹äºˆæ›´é«˜çš„æƒé‡ã€‚
*   **`label_smoothing`**ï¼šPyTorch æ–°ç‰ˆæœ¬æ”¯æŒï¼Œé˜²æ­¢æ¨¡å‹è¿‡åº¦è‡ªä¿¡ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚

#### ğŸ’» å®Œæ•´ç¤ºä¾‹
```python
import torch
import torch.nn as nn

# 3ä¸ªæ ·æœ¬ï¼Œ5ä¸ªç±»åˆ«
input_logits = torch.randn(3, 5, requires_grad=True)
# çœŸå®æ ‡ç­¾ç´¢å¼•
target_indices = torch.tensor([1, 0, 4], dtype=torch.long)

# ä½¿ç”¨ç±»åˆ«æƒé‡å¤„ç†ä¸å¹³è¡¡
class_weights = torch.tensor([1.0, 2.0, 1.0, 1.0, 1.0]) 
criterion = nn.CrossEntropyLoss(weight=class_weights)

loss = criterion(input_logits, target_indices)
loss.backward()

print(f"CE Loss: {loss.item()}")
```

---

### 2.3 `nn.BCELoss` vs `nn.BCEWithLogitsLoss`

è¿™ä¸¤ä¸ªç”¨äº **äºŒåˆ†ç±»**ï¼ˆBinary Classificationï¼‰æˆ– **å¤šæ ‡ç­¾åˆ†ç±»**ï¼ˆMulti-label Classificationï¼‰ã€‚

*   **`nn.BCELoss`**:
    *   **è¦æ±‚**: è¾“å…¥ $x$ å¿…é¡»åœ¨ $[0, 1]$ ä¹‹é—´ï¼ˆé€šå¸¸ç»è¿‡ Sigmoidï¼‰ã€‚
    *   **é™·é˜±**: å¦‚æœ $x$ éå¸¸æ¥è¿‘ 0 æˆ– 1ï¼Œ$\log$ ä¼šè¶‹å‘è´Ÿæ— ç©·ï¼Œå¯¼è‡´æ•°å€¼ä¸ç¨³å®šã€‚

*   **`nn.BCEWithLogitsLoss` (æ¨è âœ…)**:
    *   **æœºåˆ¶**: å†…éƒ¨é›†æˆäº† Sigmoid + BCELossã€‚
    *   **ä¼˜åŠ¿**: ä½¿ç”¨ Log-Sum-Exp æŠ€å·§ä¿è¯æ•°å€¼ç¨³å®šæ€§ã€‚**æ°¸è¿œä¼˜å…ˆä½¿ç”¨è¿™ä¸ªï¼Œè€Œä¸æ˜¯ Sigmoid + BCELoss**ã€‚
    *   **Pos Weight**: æ”¯æŒ `pos_weight` å‚æ•°ï¼Œä¸“é—¨ç”¨äºè°ƒæ•´æ­£æ ·æœ¬çš„æƒé‡ï¼Œæ¯”æ‰‹åŠ¨åŠ æƒæ›´æ–¹ä¾¿ã€‚

#### ğŸ’» å®Œæ•´ç¤ºä¾‹ (BCEWithLogitsLoss)
```python
import torch
import torch.nn as nn

# é¢„æµ‹å€¼ (Logits, ä¸éœ€è¦ Sigmoid)
logits = torch.randn(3, requires_grad=True)
# ç›®æ ‡å€¼ (Float, 0.0 æˆ– 1.0)
targets = torch.empty(3).random_(2)

# pos_weight: å‡è®¾æ­£æ ·æœ¬å¾ˆå°‘ï¼Œç»™æ­£æ ·æœ¬ 3å€æƒé‡
pos_weight = torch.tensor([3.0])
criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

loss = criterion(logits, targets)
loss.backward()
print(f"BCE With Logits Loss: {loss.item()}")
```

---

### 2.4 å¤šæ ‡ç­¾ä¸ Margin æŸå¤±

*   **`nn.MultiLabelSoftMarginLoss`**:
    *   æœ¬è´¨ä¸Šæ˜¯ `BCEWithLogitsLoss` çš„å¤šæ ‡ç­¾ç‰ˆæœ¬ã€‚å¯¹æ¯ä¸ªç±»åˆ«ç‹¬ç«‹è®¡ç®— BCEï¼Œç„¶åå–å¹³å‡ã€‚
    *   **é€‚ç”¨**: ä¸€å¼ å›¾ç‰‡å¯èƒ½åŒæ—¶æœ‰â€œçŒ«â€å’Œâ€œç‹—â€æ ‡ç­¾ã€‚
*   **`nn.MultiLabelMarginLoss`**:
    *   åŸºäº Hinge Loss çš„å¤šæ ‡ç­¾æŸå¤±ã€‚
    *   **é€‚ç”¨**: æ­¤æ—¶å…³æ³¨çš„æ˜¯â€œæ­£ç±»åˆ«çš„å¾—åˆ†â€è¦æ¯”â€œè´Ÿç±»åˆ«çš„å¾—åˆ†â€é«˜å‡ºä¸€ä¸ª Marginã€‚

---

## 3. åµŒå…¥ä¸æ’åºæŸå¤± (Embedding & Ranking Losses) ğŸ”—

è¿™ç±»æŸå¤±å‡½æ•°å¸¸ç”¨äº Metric Learningï¼ˆåº¦é‡å­¦ä¹ ï¼‰ã€Siamese Networkï¼ˆå­ªç”Ÿç½‘ç»œï¼‰ã€Face Recognitionï¼ˆäººè„¸è¯†åˆ«ï¼‰ç­‰ã€‚

### 3.1 `nn.MarginRankingLoss`

#### ğŸ“ æ•°å­¦å®šä¹‰
ç»™å®šä¸¤ä¸ªè¾“å…¥ $x_1, x_2$ å’Œæ ‡ç­¾ $y \in \{1, -1\}$ï¼ˆ1 è¡¨ç¤º $x_1$ æ’ååº”é«˜äº $x_2$ï¼‰ï¼š

$$
\text{loss}(x_1, x_2, y) = \max(0, -y \cdot (x_1 - x_2) + \text{margin})
$$

#### ğŸ› ï¸ é€‚ç”¨åœºæ™¯
*   Learning to Rankï¼ˆæ’åºå­¦ä¹ ï¼‰ã€‚
*   çŸ¥è¯†å›¾è°±åµŒå…¥ï¼ˆKnowledge Graph Embeddingï¼Œå¦‚ TransEï¼‰ã€‚

#### ğŸ’» å®Œæ•´ç¤ºä¾‹
```python
import torch
import torch.nn as nn

# x1 å¾—åˆ†, x2 å¾—åˆ†
x1 = torch.randn(3, requires_grad=True)
x2 = torch.randn(3, requires_grad=True)
# y=1 æ„å‘³ç€ x1 åº”è¯¥ > x2
y = torch.tensor([1, -1, 1]) 

criterion = nn.MarginRankingLoss(margin=0.1)
loss = criterion(x1, x2, y)
loss.backward()
```

---

### 3.2 `nn.CosineEmbeddingLoss`

#### ğŸ“ æ•°å­¦å®šä¹‰
ç”¨äºå­¦ä¹ ä¸¤ä¸ªå‘é‡æ˜¯å¦ç›¸ä¼¼ã€‚åŸºäºä½™å¼¦ç›¸ä¼¼åº¦ã€‚

$$
\text{loss}(x_1, x_2, y) = \begin{cases}
1 - \cos(x_1, x_2) & \text{if } y = 1 \\
\max(0, \cos(x_1, x_2) - \text{margin}) & \text{if } y = -1
\end{cases}
$$

#### ğŸ› ï¸ é€‚ç”¨åœºæ™¯
*   æ–‡æœ¬è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚
*   è‡ªç›‘ç£å­¦ä¹ ï¼ˆSelf-Supervised Learningï¼‰ã€‚

> âš ï¸ **å‘ç‚¹**ï¼šè¿™é‡Œçš„ margin æ˜¯é’ˆå¯¹ $\cos$ å€¼çš„ã€‚å½“ $y=-1$ï¼ˆä¸ç›¸ä¼¼ï¼‰æ—¶ï¼Œæˆ‘ä»¬å¸Œæœ› $\cos$ å€¼å°äº marginã€‚é€šå¸¸ margin è®¾ä¸º 0.0 åˆ° 0.5 ä¹‹é—´ã€‚

---

### 3.3 `nn.TripletMarginLoss`

**å…¨ç§°**: ä¸‰å…ƒç»„æŸå¤±

#### ğŸ“ æ•°å­¦å®šä¹‰
è¾“å…¥ä¸‰å…ƒç»„ï¼šAnchor ($a$), Positive ($p$), Negative ($n$)ã€‚

$$
L(a, p, n) = \max(0, d(a, p) - d(a, n) + \text{margin})
$$

å…¶ä¸­ $d(\cdot)$ é€šå¸¸æ˜¯ L2 è·ç¦»ã€‚ç›®æ ‡æ˜¯æ‹‰è¿‘ $(a, p)$ï¼Œæ¨è¿œ $(a, n)$ï¼Œä¸”ä¸¤è€…è·ç¦»å·®è‡³å°‘ä¸º marginã€‚

#### ğŸ› ï¸ é€‚ç”¨åœºæ™¯
*   äººè„¸è¯†åˆ«ï¼ˆFaceNetï¼‰ã€‚
*   è¡Œäººé‡è¯†åˆ«ï¼ˆReIDï¼‰ã€‚

> ğŸ’¡ **æœ€ä½³å®è·µ**ï¼š**éš¾æ ·æœ¬æŒ–æ˜ (Hard Negative Mining)** è‡³å…³é‡è¦ã€‚éšæœºé‡‡æ ·çš„ä¸‰å…ƒç»„å¾€å¾€å¾ˆå®¹æ˜“æ»¡è¶³ margin æ¡ä»¶ï¼ˆLoss=0ï¼‰ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ã€‚

---

## 4. åˆ†å¸ƒæ•£åº¦ä¸åºåˆ—æŸå¤± (Distribution & Sequence Losses) ğŸ“Š

### 4.1 `nn.KLDivLoss`

**å…¨ç§°**ï¼šKullback-Leibler Divergence Loss

#### ğŸ“ æ•°å­¦å®šä¹‰
$$
L(y_{\text{pred}}, y_{\text{true}}) = y_{\text{true}} \cdot (\log y_{\text{true}} - y_{\text{pred}})
$$

#### âš ï¸ å…³é”®é™·é˜±
*   **è¾“å…¥æ ¼å¼**:
    *   `input` ($y_{\text{pred}}$): å¿…é¡»æ˜¯ **Log-Probabilities** (LogSoftmax çš„è¾“å‡º)ã€‚
    *   `target` ($y_{\text{true}}$): å¿…é¡»æ˜¯ **Probabilities** (æ™®é€šçš„æ¦‚ç‡åˆ†å¸ƒ)ã€‚
*   **Reduction**:
    *   `reduction='mean'`: è¿”å›é€å…ƒç´ çš„å¹³å‡å€¼ï¼ˆæ•°å­¦ä¸Šä¸å¯¹åº” KL æ•£åº¦ï¼‰ã€‚
    *   `reduction='batchmean'`: **å¿…é¡»ä½¿ç”¨è¿™ä¸ª**ï¼Œæ‰èƒ½å¾—åˆ°æ•°å­¦æ„ä¹‰ä¸Šæ­£ç¡®çš„ KL æ•£åº¦ï¼ˆæŒ‰ Batch å½’ä¸€åŒ–ï¼‰ã€‚

#### ğŸ’» å®Œæ•´ç¤ºä¾‹
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# é¢„æµ‹åˆ†å¸ƒ (Logits -> LogSoftmax)
logits = torch.randn(2, 5, requires_grad=True)
input_log_probs = F.log_softmax(logits, dim=1)

# çœŸå®åˆ†å¸ƒ (æ¦‚ç‡)
target_probs = F.softmax(torch.randn(2, 5), dim=1)

# å¿…é¡»ä½¿ç”¨ batchmean
criterion = nn.KLDivLoss(reduction='batchmean')
loss = criterion(input_log_probs, target_probs)

print(f"KL Div Loss: {loss.item()}")
```

---

### 4.2 `nn.CTCLoss`

**å…¨ç§°**: Connectionist Temporal Classification

#### ğŸ› ï¸ é€‚ç”¨åœºæ™¯
*   åºåˆ—å¯¹åºåˆ—ä»»åŠ¡ï¼Œä½†è¾“å…¥è¾“å‡ºé•¿åº¦ä¸ä¸€è‡´ä¸”æ— éœ€å¯¹é½ã€‚
*   å…¸å‹åº”ç”¨ï¼š**è¯­éŸ³è¯†åˆ« (ASR)**ã€**OCR**ã€‚

#### ğŸ“ æ ¸å¿ƒæ¦‚å¿µ
*   å¼•å…¥ `blank` æ ‡ç­¾ã€‚
*   è§£å†³ $X$ (éŸ³é¢‘å¸§) åˆ° $Y$ (æ–‡æœ¬å­—ç¬¦) çš„æ˜ å°„ï¼Œå…¶ä¸­ len(X) >> len(Y)ã€‚

---

## 5. æ€»ç»“ä¸é€ŸæŸ¥è¡¨ ğŸ“

åœ¨é€‰æ‹©æŸå¤±å‡½æ•°æ—¶ï¼Œè¯·éµå¾ªä»¥ä¸‹å†³ç­–æ ‘ï¼š

1.  **æ˜¯å›å½’é—®é¢˜ï¼Ÿ**
    *   æ™®é€šå›å½’ $\rightarrow$ `MSELoss`
    *   æœ‰å¼‚å¸¸å€¼ $\rightarrow$ `HuberLoss` / `L1Loss`
2.  **æ˜¯åˆ†ç±»é—®é¢˜ï¼Ÿ**
    *   äºŒåˆ†ç±» $\rightarrow$ `BCEWithLogitsLoss` (å¿…é€‰)
    *   å¤šåˆ†ç±» $\rightarrow$ `CrossEntropyLoss` (å¿…é€‰)
3.  **æ˜¯Embeddingå­¦ä¹ ï¼Ÿ**
    *   é…å¯¹æ•°æ® $\rightarrow$ `CosineEmbeddingLoss`
    *   ä¸‰å…ƒç»„æ•°æ® $\rightarrow$ `TripletMarginLoss`

### é™„å½•ï¼šPyTorch Loss é€ŸæŸ¥è¡¨

| Loss Function | Input Shape | Target Shape | Activation Required | å…¸å‹åº”ç”¨ |
| :--- | :--- | :--- | :--- | :--- |
| `MSELoss` | $(N, *)$ | $(N, *)$ | None | å›å½’ |
| `L1Loss` | $(N, *)$ | $(N, *)$ | None | é²æ£’å›å½’ |
| `CrossEntropy`| $(N, C)$ | $(N)$ (Index) | None (Logits) | å¤šåˆ†ç±» |
| `NLLLoss` | $(N, C)$ | $(N)$ (Index) | LogSoftmax | å¤šåˆ†ç±» |
| `BCEWithLogits`| $(N, *)$ | $(N, *)$ | None (Logits) | äºŒåˆ†ç±» |
| `KLDivLoss` | $(N, *)$ | $(N, *)$ | LogSoftmax | çŸ¥è¯†è’¸é¦ |

---

## 6. å‚è€ƒæ–‡çŒ®

1.  [PyTorch Official Documentation: torch.nn](https://pytorch.org/docs/stable/nn.html#loss-functions)
2.  Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT press.

---
*Created by AI Assistant using PyTorch 2.x standards. Last updated: 2026-02-25.*
